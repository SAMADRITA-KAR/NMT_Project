{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"d5jkALjUMX05","outputId":"1f50ab30-60d8-42a5-a959-8cc8d5b38002"},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Command '1' unrecognized\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","\n","Packages:\n","  [ ] abc................. Australian Broadcasting Commission 2006\n","  [ ] alpino.............. Alpino Dutch Treebank\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] bcp47............... BCP-47 Language Tags\n","  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n","                           Extraction Systems in Biology)\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] brown............... Brown Corpus\n","  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n","  [ ] cess_cat............ CESS-CAT Treebank\n","  [ ] cess_esp............ CESS-ESP Treebank\n","  [ ] chat80.............. Chat-80 Data Files\n","  [ ] city_database....... City Database\n","  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n","  [ ] comparative_sentences Comparative Sentence Dataset\n","  [ ] comtrans............ ComTrans Corpus Sample\n","  [ ] conll2000........... CONLL 2000 Chunking Corpus\n","  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n","                           and Basque Subset)\n","  [ ] crubadan............ Crubadan Corpus\n","  [ ] dependency_treebank. Dependency Parsed Treebank\n","  [ ] dolch............... Dolch Word List\n","  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n","                           Corpus\n","  [ ] extended_omw........ Extended Open Multilingual WordNet\n","  [ ] floresta............ Portuguese Treebank\n","  [ ] framenet_v15........ FrameNet 1.5\n","  [ ] framenet_v17........ FrameNet 1.7\n","  [ ] gazetteers.......... Gazeteer Lists\n","  [ ] genesis............. Genesis Corpus\n","  [ ] gutenberg........... Project Gutenberg Selections\n","  [ ] ieer................ NIST IE-ER DATA SAMPLE\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","  [ ] kimmo............... PC-KIMMO Data Files\n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw-1.4............. Open Multilingual Wordnet\n","  [ ] omw................. Open Multilingual Wordnet\n","  [ ] opinion_lexicon..... Opinion Lexicon\n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [ ] punkt............... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet2021......... Open English Wordnet 2021\n","  [ ] wordnet2022......... Open English Wordnet 2022\n","  [ ] wordnet31........... Wordnet 3.1\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","\n","Collections:\n","  [ ] all-corpora......... All the corpora\n","  [ ] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [ ] all................. All packages\n","  [ ] book................ Everything used in the NLTK Book\n","  [ ] popular............. Popular packages\n","  [ ] tests............... Packages for running tests\n","  [ ] third-party......... Third-party data packages\n","\n","([*] marks installed packages)\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n"]}],"source":["# Load the text\n","filename = 'HP.docx'\n","\n","# Open the model file as read only\n","file = open(filename, 'rt')\n","file.close()\n","\n","import nltk\n","nltk.download()\n","from nltk.tokenize import word_tokenize\n","tokens = word_tokenize(text)\n","# Stem the words\n","from nltk.stem.porter import PorterStemmer\n","porter = PorterStemmer()\n","st = [porter.stem(word) for word in tokens]\n","print(st[:91156])\n","# Convert the words to lower case\n","line = [word.lower() for word in line]\n","# Remove punctuation from each word\n","import string\n","table = str.maketrans('', '', string.punctuation)\n","stripped = [word.translate(table) for word in tokens]\n","# Remove all tokens that are not alphabetic\n","words = [word for word in tokens if word.isalpha()]\n","# Store as string\n","def HP():\n","  from numpy import array\n","  clean_pair.append(' '.join(line))\n","  cleaned.append(clean_pair)\n","  return array(cleaned)\n","\n","  # Save a list of clean sentences to the model file\n","from pickle import dump\n","def save_clean_data(sentences, filename):\n","    dump(sentences, open(filename, 'wb'))\n","    print('Saved: %s' % filename)\n","\n","# Load dataset\n","filename = 'English-Bengali Dataset.zip'\n","doc = load_doc(filename)\n","\n","# Split into english-bengali pairs\n","pairs = to_pairs(doc)\n","\n","# Clean sentences\n","clean_pairs = clean_pairs(pairs)\n","\n","# Save clean pairs to file\n","save_clean_data(clean_pairs, 'HP.docx')\n","\n","# Spot check\n","for i in range(91156):\n"," print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n","\n","from pickle import load\n","from pickle import dump\n","\n","# Load a clean dataset\n","def load_clean_sentences():\n","    return load(open(filename, 'rb'))\n","\n","# Save a list of clean sentences to the model file\n","def save_clean_data(sentences, filename):\n","    dump(sentences, open(filename, 'wb'))\n","    print('Saved: %s' % filename)\n","\n","\n","# Load dataset\n","raw_dataset = load_clean_sentences('English-Bengali Dataset.zip')\n","\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","\n","# Load a clean dataset\n","def load_clean_sentences(dataset):\n","    return load(open(dataset, 'rb'))\n","\n","# Fit a tokenizer\n","def create_tokenizer(lines):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(lines)\n","    return tokenizer\n","\n","# Find length of the longest sequence of tokens\n","def max_length(lines):\n","    return max(len(line.split()) for line in lines)\n","\n","# Encode and Zero Pad the sequences\n","def encode_sequences(tokenizer, length, lines):\n","    # integer encode sequences\n","    X = tokenizer.texts_to_sequences(lines)\n","    # pad sequences with 0 values\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    return X\n","\n","# One hot encode the sequences\n","def encode_output(sequences, vocab_size):\n","    ylist = list()\n","    for sequence in sequences:\n","        encoded = to_categorical(sequence, num_classes=vocab_size)\n","        ylist.append(encoded)\n","    y = array(ylist)\n","    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","    return y\n","\n","# Define NMT model file\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","    model = Sequential()\n","    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","    model.add(LSTM(n_units))\n","    model.add(RepeatVector(tar_timesteps))\n","    model.add(LSTM(n_units, return_sequences=True))\n","    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","    return model\n","\n","# Define the model file\n","model = define_model(ben_vocab_size, eng_vocab_size, ben_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Map an integer to a word\n","def word_for_id(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None\n","\n","# Generate the target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","    prediction = model.predict(source)\n","    integers = [argmax(vector) for vector in prediction]\n","    target = list()\n","    for i in integers:\n","        word = word_for_id(i, tokenizer)\n","        if word is None:\n","            break\n","        target.append(word)\n","    return ' '.join(target)\n","\n","# Evaluate the skill of the model file\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n"," actual, predicted = list(), list()\n"," for i, source in enumerate(sources):\n","  source = source.reshape((1, source.shape[0]))\n","  translation = predict_sequence(model, eng_tokenizer, source)\n","  raw_target, raw_src = raw_dataset[i]\n","  if i < 10:\n","    print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","    actual.append([raw_target.split()])\n","    predicted.append(translation.split())\n","\n","# Load datasets\n","dataset = load_clean_sentences('English-Bengali Dataset.zip')\n","train = load_clean_sentences('English-Bengali Dataset.zip')\n","test = load_clean_sentences('English-Bengali Dataset.zip')\n","\n","# Prepare English tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:91156])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:91156, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","\n","# Prepare Bengali tokenizer\n","ben_tokenizer = create_tokenizer(dataset[:91156])\n","ben_vocab_size = len(ben_tokenizer.word_index) + 1\n","ben_length = max_length(dataset[:91156])\n","print('Bengali Vocabulary Size: %d' % ben_vocab_size)\n","print('Bengali Max Length: %d' % (ben_length))\n","\n","# Prepare the training data\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:91156])\n","trainY = encode_output(trainY, eng_vocab_size)\n","\n","# Prepare the test data\n","testY = encode_sequences(eng_tokenizer, eng_length, test[91156:])\n","testY = encode_output(testY, eng_vocab_size)\n","\n","# Load the model file\n","model = load_model('HP.docx')\n","\n","# Test on some training sequences\n","print('Train')\n","evaluate_model(model, ben_tokenizer, trainY)\n","\n","# Test on some testing sequences\n","print('Test')\n","evaluate_model(model, ben_tokenizer, testY);"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}